{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db888414",
   "metadata": {},
   "source": [
    "# Project : INTRODUCTION TO MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222eb5c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import math \n",
    "import random \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085dd70d",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9482591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_csv(\"waveform.data.csv\", sep=',', header=None)\n",
    "\n",
    "dataset_shuffle = dataset.sample(frac=1.0, random_state=42)\n",
    "\n",
    "X = dataset_shuffle.iloc[:, :-1]\n",
    "y = dataset_shuffle.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156ad94",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bdcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X.iloc[:4000]\n",
    "y_train = y.iloc[:4000]\n",
    "\n",
    "X_test = X.iloc[4000:]\n",
    "y_test = y.iloc[4000:]\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "def distance_euclidienne(point1, point2):\n",
    "    \"\"\"\n",
    "    Calcule la distance euclidienne entre deux points.\n",
    "    \"\"\"\n",
    "    somme_carres = 0.0\n",
    "    for i in range(len(point1)):\n",
    "        difference = point1[i] - point2[i]\n",
    "        somme_carres += difference ** 2\n",
    "    return somme_carres ** 0.5\n",
    "\n",
    "def knn_algorithm(x, k, metrique, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Prédit la classe d'un point x en utilisant l'algorithme k-NN.\n",
    "    \"\"\"\n",
    "    list_distances = []\n",
    "    \n",
    "    # Convertir x en tableau numpy 2D pour cdist\n",
    "    x_2d = np.array(x).reshape(1, -1)\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # dist = distance_euclidienne(x, X_train.iloc[i].values)\n",
    "        \n",
    "        # Méthode 2 : Avec cdist (plus rapide)\n",
    "        point_2d = np.array(X_train.iloc[i]).reshape(1, -1)\n",
    "        dist = cdist(x_2d, point_2d, metric=metrique)[0][0]\n",
    "        \n",
    "        list_distances.append((dist, i))\n",
    "\n",
    "    list_distances.sort(key=lambda x: x[0])  # Trie par distance\n",
    "\n",
    "    k_nearest_neighbors = list_distances[:k]\n",
    "    votes = {}\n",
    "    \n",
    "    for dist, index in k_nearest_neighbors:\n",
    "        label = y_train.iloc[index]\n",
    "        votes[label] = votes.get(label,0) + 1\n",
    "\n",
    "    return max(votes, key=votes.get)\n",
    "\n",
    "\n",
    "\"\"\" Optimisation KNN en vectorisant nos données et en effectuant les 4 millions de calculs de distance en seulement une opération avec Numpy \"\"\"\n",
    "\n",
    "def knn_algorithm_vectorise(x, k, metrique, X_train, y_train):\n",
    "    x_2d = np.array(x).reshape(1, -1)\n",
    "    X_2d = np.array(X_train)\n",
    "    \n",
    "    distances = cdist(x_2d, X_2d, metric=metrique)[0]\n",
    "    \n",
    "    k_indices = np.argsort(distances)[:k]\n",
    "    votes = y_train.iloc[k_indices].value_counts()\n",
    "    return votes.idxmax()\n",
    "\n",
    "def evaluer_modele(X_test, y_test, k, metrique, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Évalue la performance du modèle kNN sur un ensemble de test\n",
    "    \"\"\"\n",
    "    nombre_correct = 0\n",
    "    total = len(X_test)\n",
    "    \n",
    "    for i in range(total):\n",
    "        point_test = X_test.iloc[i].values  # Convertir en numpy array\n",
    "        vrai_label = y_test.iloc[i]\n",
    "        \n",
    "        # prediction = knn_algorithm(point_test, k, metrique, X_train, y_train)\n",
    "        prediction = knn_algorithm_vectorise(point_test, k, metrique, X_train, y_train)\n",
    "        if prediction == vrai_label:\n",
    "            nombre_correct += 1\n",
    "    \n",
    "    return nombre_correct / total\n",
    "\n",
    "def cross_validation_knn(X, y, k_values, n_folds=5, metric='euclidean'):\n",
    "    \"\"\"\n",
    "    Cross-validation pour choisir le meilleur k\n",
    "\n",
    "    Args:\n",
    "        X: Features du dataset\n",
    "        y: Labels du dataset complet\n",
    "        k_values: Liste contenant les valeurs de k à tester\n",
    "        n_folds: Nombre de folds\n",
    "        metric: Métrique de distance\n",
    "\n",
    "    Returns:\n",
    "        best_k: Meilleure valeur de k\n",
    "        best_accuracy: Meilleure précision moyenne\n",
    "        results: Dictionnaire avec les résultats pour chaque k\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {k: [] for k in k_values}\n",
    "    \n",
    "    # création des folds\n",
    "    fold_size = len(X) // n_folds\n",
    "    indices = np.arange(len(X))\n",
    "    \n",
    "    # mélanger les indices pour des folds aléatoires\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    print(f\"Début de la cross-validation ({n_folds} folds) pour les k: {k_values}\")\n",
    "    \n",
    "    # Pour chaque fold\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"fold {fold + 1}/{n_folds}...\")\n",
    "        \n",
    "        # Définition des indices pour validation et entraînement\n",
    "        val_start = fold * fold_size\n",
    "        val_end = (fold + 1) * fold_size\n",
    "        val_indices = indices[val_start:val_end]\n",
    "        train_indices = np.concatenate([indices[:val_start], indices[val_end:]])\n",
    "        \n",
    "        # créer des datasets pour ce fold\n",
    "        X_train_fold = X.iloc[train_indices].reset_index(drop=True)\n",
    "        y_train_fold = y.iloc[train_indices].reset_index(drop=True)\n",
    "        X_val_fold = X.iloc[val_indices].reset_index(drop=True)\n",
    "        y_val_fold = y.iloc[val_indices].reset_index(drop=True)\n",
    "        \n",
    "        # evaluation pour chaque valeur de k\n",
    "        for k in k_values:\n",
    "            accuracy = evaluer_modele(X_val_fold, y_val_fold, k, metric, X_train_fold, y_train_fold)\n",
    "            results[k].append(accuracy)\n",
    "    \n",
    "    # calcul des moyennes et choix du meilleur k\n",
    "    best_accuracy = 0\n",
    "    best_k = None\n",
    "    mean_accuracies = {}\n",
    "    \n",
    "    for k in k_values:\n",
    "        mean_accuracy = np.mean(results[k])\n",
    "        mean_accuracies[k] = mean_accuracy\n",
    "        if mean_accuracy > best_accuracy:\n",
    "            best_accuracy = mean_accuracy\n",
    "            best_k = k\n",
    "    \n",
    "    return best_k, best_accuracy, mean_accuracies\n",
    "    \n",
    "\n",
    "# Utilisation\n",
    "def plot_results(mean_accuracies):\n",
    "    \"\"\"Trace les résultats de la cross-validation\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    k_values = list(mean_accuracies.keys())\n",
    "    accuracies = list(mean_accuracies.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, accuracies, 'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Valeur de k')\n",
    "    plt.ylabel('Accuracy moyenne')\n",
    "    plt.title('Performance du kNN en fonction de k (Cross-Validation)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(k_values)\n",
    "    \n",
    "    # marquer en rouge le meilleur k\n",
    "    best_k = max(mean_accuracies, key=mean_accuracies.get)\n",
    "    best_acc = mean_accuracies[best_k]\n",
    "    plt.plot(best_k, best_acc, 'ro', markersize=10, label=f'Meilleur k: {best_k} ({best_acc:.3f})')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef31703",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "~ 2-4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # utilise seulement les 4000 points d'entraînement pour la CV\n",
    "    k_values = list(range(1, 51))\n",
    "    \n",
    "    best_k, best_accuracy, mean_accuracies = cross_validation_knn(\n",
    "        X_train, y_train, k_values, n_folds=5, metric='euclidean'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n Meilleur k: {best_k}, précision de {best_accuracy:.4f}\")\n",
    "    \n",
    "    plot_results(mean_accuracies)\n",
    "    \n",
    "    # test set avec le meilleur k\n",
    "    final_accuracy = evaluer_modele(X_test, y_test, best_k, 'euclidean', X_train, y_train)\n",
    "    print(f\"Accuracy finale sur le test set: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\", + \"avec k k={best_k}\")\n",
    "    print(f\"Bayes optimal: 86%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b50c4c7",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "accuracy = evaluer_modele(X_test, y_test, 5, 'euclidean', X_train, y_train)\n",
    "print(f\"Avec k=5, accuracy = {accuracy:.3f} ({accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d320ef14",
   "metadata": {},
   "source": [
    "## Data reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1,v2):\n",
    "    \"\"\"\n",
    "    Renvoie la distance euclidiennes entre les vecteurs v1 et v2\n",
    "    \"\"\"\n",
    "    somme = 0. \n",
    "    for x, y in zip(v1, v2):\n",
    "        somme += (x - y) ** 2\n",
    "    return math.sqrt(somme)\n",
    "\n",
    "def un_NN(p, X, y): # TODO à optimiser en se servant des distances déjà calculées pour le k-NN \n",
    "    \"\"\"\n",
    "    1-NN \n",
    "\n",
    "    Args:\n",
    "        p: Point à tester\n",
    "        X: Features du dataset\n",
    "        y: Labels du dataset \n",
    "\n",
    "    Returns:\n",
    "        label: label du point le plus proche \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation \n",
    "    dmin = distance(p, X[0])\n",
    "    label = y[0]\n",
    "\n",
    "    for i in range(1, X.shape[0]):\n",
    "        d = distance(p, X[i])\n",
    "        if d < dmin: # si on trouve un point plus proche \n",
    "            dmin = d\n",
    "            label = y[i]\n",
    "    \n",
    "    return label\n",
    "\n",
    "def sep_data(X, y):\n",
    "    \"\"\"\n",
    "    Mélange et sépare les données \n",
    "\n",
    "    Args:\n",
    "        X: Features du dataset\n",
    "        y: Labels du dataset \n",
    "\n",
    "    Returns:\n",
    "        l_id: Liste d'indices \n",
    "        milieu: indice du milieu\n",
    "    \"\"\"\n",
    "    l_id = [i for i in range(X.shape[0])]\n",
    "    random.shuffle(l_id)\n",
    "    milieu = len(l_id) // 2\n",
    "    return (l_id, milieu)\n",
    "\n",
    "def reduction1(X, y):\n",
    "    \"\"\"\n",
    "    1ère algorithme de réduction du dataset vu en cours \n",
    "    Suppression de la zone de biais \n",
    "    \n",
    "    Args:\n",
    "        X: Features du dataset\n",
    "        y: Labels du dataset \n",
    "\n",
    "    Returns:\n",
    "        X_cleaned, y_cleaned: Dataset après réduction \n",
    "    \"\"\"\n",
    "\n",
    "    # Split randomly S into two subsets S1 and S2;\n",
    "    (ordre, milieu) = sep_data(X, y)\n",
    "    S1_X, S1_y = X[ordre[:milieu]], y[ordre[:milieu]]\n",
    "    S2_X, S2_y = X[ordre[milieu:]], y[ordre[milieu:]]\n",
    "\n",
    "    ch = True \n",
    "    while (ch): # Tant qu'il y a un changement S1 et S2 ne sont pas stabilisées \n",
    "        ch = False\n",
    "\n",
    "        # Classify S1 with S2 using the 1-NN rule;\n",
    "        # Remove from S1 the misclassified instances;\n",
    "        nv_S1_X = []\n",
    "        nv_S1_y = []\n",
    "        for xi, yi in zip(S1_X, S1_y):\n",
    "            l = un_NN(xi, S2_X, S2_y)\n",
    "            if l == yi: \n",
    "                nv_S1_X.append(xi)\n",
    "                nv_S1_y.append(yi)\n",
    "            else:\n",
    "                ch = True # pas encore stable car mal classé \n",
    "        S1_X = np.array(nv_S1_X)\n",
    "        S1_y = np.array(nv_S1_y)\n",
    "\n",
    "        # Classify S2 with the new set S1 using the 1-NN rule;\n",
    "        # Remove from S2 the misclassified instances;\n",
    "        nv_S2_X = []\n",
    "        nv_S2_y = []\n",
    "        for xi, yi in zip(S2_X, S2_y):\n",
    "            l = un_NN(xi, S1_X, S1_y)\n",
    "            if l == yi: \n",
    "                nv_S2_X.append(xi)\n",
    "                nv_S2_y.append(yi)\n",
    "            else:\n",
    "                ch = True # pas encore stable car mal classé \n",
    "        S2_X = np.array(nv_S2_X)\n",
    "        S2_y = np.array(nv_S2_y)\n",
    "    \n",
    "    # Union de S1 et S2 \n",
    "    X_cleaned = np.concatenate((S1_X, S2_X), axis=0)\n",
    "    y_cleaned = np.concatenate((S1_y, S2_y))\n",
    "    \n",
    "    return (X_cleaned, y_cleaned)\n",
    "\n",
    "def reduction2(X, y):\n",
    "    \"\"\"\n",
    "    2ème algorithme de réduction du dataset vu en cours \n",
    "    Suppression des données non pertinentes \n",
    "    \n",
    "    Args:\n",
    "        X: Features du dataset\n",
    "        y: Labels du dataset \n",
    "\n",
    "    Returns:\n",
    "        X_cleaned, y_cleaned: Dataset après réduction \n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation\n",
    "    STORAGE_X = []\n",
    "    STORAGE_y = []\n",
    "    DUSTBIN_X = []\n",
    "    DUSTBIN_y = []\n",
    "\n",
    "    # Draw randomly a training example from S and put it in STORAGE;\n",
    "    idx0 = random.randint(0, X.shape[0] - 1)\n",
    "    STORAGE_X.append(X[idx0])\n",
    "    STORAGE_y.append(y[idx0])\n",
    "\n",
    "    ch = True\n",
    "    while (ch): # Tant que STORAGE n'est pas stabilisé \n",
    "        ch = False\n",
    "        \n",
    "        for xi, yi in zip(X, y):\n",
    "\n",
    "            # Ne pas traiter un point déjà dans STORAGE \n",
    "            liste_STORAGE_X = [p.tolist() for p in STORAGE_X]\n",
    "            if xi.tolist() in liste_STORAGE_X:\n",
    "                continue\n",
    "\n",
    "            l = un_NN(xi, np.array(STORAGE_X), np.array(STORAGE_y))\n",
    "\n",
    "            if l == yi: # if xi is correctly classified with STORAGE using the 1-NN rule\n",
    "                # bien classé -> poubelle (inutile)\n",
    "                DUSTBIN_X.append(xi)\n",
    "                DUSTBIN_y.append(yi)\n",
    "            else:\n",
    "                # mal classé -> stockage (utile)\n",
    "                STORAGE_X.append(xi)\n",
    "                STORAGE_y.append(yi)\n",
    "                ch = True\n",
    "\n",
    "    return (np.array(STORAGE_X), np.array(STORAGE_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f614b",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3477f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X, y, titre):\n",
    "    \"\"\"\n",
    "    PCA\n",
    "    \n",
    "    Args:\n",
    "        X: Features du dataset\n",
    "        y: Labels du dataset \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    X_pca = np.concatenate((X, y.reshape((y.shape[0], 1))), axis=1)\n",
    "\n",
    "    labels = X_pca[:, -1].astype(int)\n",
    "    X_features = X_pca[:, :-1]\n",
    "\n",
    "    # Centrer les données (features)\n",
    "    X_meaned = X_features - np.mean(X_features, axis=0)\n",
    "\n",
    "    cov_mat = np.cov(X_meaned, rowvar=False)\n",
    "\n",
    "    # Valeurs propres et vecteurs propres\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "    # Tri décroissant\n",
    "    sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "    sorted_eigenvectors = eigenvectors[:, sorted_index]\n",
    "\n",
    "    # Projection sur les 2 premiers vecteurs propres\n",
    "    eigenvector_subset = sorted_eigenvectors[:, 0:2]\n",
    "    X_reduced = np.dot(X_meaned, eigenvector_subset)\n",
    "\n",
    "    # Affichage avec couleur par label\n",
    "    plt.figure(figsize=(8,6))\n",
    "    scatter = plt.scatter(X_reduced[:,0], X_reduced[:,1], c=labels, cmap='viridis', edgecolor='k')\n",
    "\n",
    "    # Ajouter une légende\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Labels\")\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title(titre)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b95c63",
   "metadata": {},
   "source": [
    "### Avant réduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084de751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les fonctions de réductions prennent des tableau numpy \n",
    "X_np = X.to_numpy()\n",
    "y_np = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5819a1",
   "metadata": {},
   "source": [
    "### 1ère réduction \n",
    "\n",
    "~ 5-8 minutes\n",
    "\n",
    "Passe de 5000 lignes à environ 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb053523",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_cleaned, y_cleaned) = reduction1(X_np, y_np) \n",
    "print(X.shape, \"-> X_cleaned :\", X_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdfd02f",
   "metadata": {},
   "source": [
    "### 2ème réduction\n",
    "\n",
    "~ 2-4 minutes\n",
    "\n",
    "Passe de ~4000 lignes à ~800-900 lignes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_cleaned2, y_cleaned2) = reduction2(X_cleaned, y_cleaned)\n",
    "print(X_cleaned.shape, \"-> X_cleaned2 :\", X_cleaned2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5fd94e",
   "metadata": {},
   "source": [
    "## Affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca(X_np, y_np, \"Avant réduction\")\n",
    "pca(X_cleaned, y_cleaned, \"Après suppression de la zone de bias\")\n",
    "pca(X_cleaned2, y_cleaned2, \"Après suppression des données non pertinentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8f4e1",
   "metadata": {},
   "source": [
    "## Lancement du k-NN après réduction des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a05093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
